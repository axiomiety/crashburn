---
title: "ISLR Chapter 8"
output:
  html_document:
    df_print: paged
---

# 8.3.1

setup
```{r}
library(tree)
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
Carseats <- data.frame(Carseats, High)
```

```{r}
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
```

note the _residual mean deviance_. 373 is 400-27 (number of terminal nodes)

plots!
```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

obj itself shows the splits details: split, n, deviance, yval, (yprob)
```{r}
tree.carseats
```

splitting the data into a training and testing set:
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

77% accuracy:
```{r}
(104+50)/(104+33+13+50)
```

now with pruning, let's  - we can tweak the selection criteria via `FUN`:
```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
```


plots:
```{r}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

pruning in action:
```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

```{r}
(97+58)/(97+25+20+58)
```

# 8.3.2

regression trees
```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```
splits
```{r}
tree.boston
```

plots!
```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```
same but with a bigger tree?
```{r}
bigtree.boston <- tree(medv ~ ., Boston, subset = train, control = tree.control(nobs = length(train), mindev = 0))
plot(bigtree.boston)
text(bigtree.boston, pretty = 0)
```
ok i need to fix that plot...

using CV for the ideal number of terminal nodes
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

pruning
```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```
unpruned trees to make predictions on the test set:
```{r}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)

```
# 8.3.3

using all 12 predictors at each split of the tree
```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, importance = TRUE)
bag.boston
```
perf is much better than the pruned tree:
```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```
size of the forest matters:
```{r}
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

this gives an indication of how important certain variables are:
```{r}
importance(rf.boston)
varImpPlot(rf.boston)
```
# 8.3.4

BART

```{r}
library(BART)
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```

```{r}
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
```

# 8.4.3

we need to plot the Gini index, entropy and classification error rate for class 1 for each region

```{r}
tree.default <- rpart(default ~ ., Default)
summary(tree.default)
```
tree looks like:
```{r}
rpart.plot(tree.default)
```
we need the details for each region:
```{r}
df <- tree.default$frame
regions <- seq(1,7)
pm1s <- ifelse(df$yval == 1, 1-df$dev/df$n, df$dev/df$n)
entropies <- -(pm1s*log(pm1s) + (1-pm1s)*log(1-pm1s))
gini <- pm1s*(1-pm1s)
misclass.rate <- 1-pmax(pm1s,1-pm1s)
```

now plot
```{r}
data <- data.frame(regions=regions, entropy=entropies, gini=gini, mrate=misclass.rate)
ggplot(data, aes(x=regions)) + geom_point(aes(y=entropy), color="red") + geom_point(aes(y=gini), color="blue") + geom_point(aes(y=mrate), color="green")
```

# 8.4.7

test classification error / number of trees for mtry = 6, ntree = 25 and ntree = 500.

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)

get_test_error <- function(mtry, ntree) {
  bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry, ntree = ntree)
  yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
  mean((yhat.bag - boston.test)^2)
}
# build a df of variables
df <- data.frame(expand.grid(seq(2,10),seq(50,500,50)))
names(df) <- c('mtry','ntree')
df['tmse'] <- mapply(get_test_error, df$mtry, df$ntree)
```

now for a plot
```{r}
library(ggplot2)
ggplot(df, aes(x=ntree,y=tmse,color=mtry)) + geom_point()
```
# 8.4.8

predicting `Sales` from `Carseats`

(a) split the data
```{r}
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
```

(b) regression tree - inc MSE
```{r}
# seems we should be doing that with rpart!
tree.carseats <- tree(Sales ~ ., Carseats, subset = train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

same thing with `rpart`:
```{r}
library(rpart)
library(rpart.plot)
tree.carseats2 <- rpart(Sales ~ ., data=Carseats, subset=train)
rpart.plot(tree.carseats2, main = "Rpart FTW", type=3)
```

test MSE
```{r}
sales_hat <- predict(tree.carseats, newdata = Carseats[-train, ])
sales.test <- Carseats[-train, "Sales"]
mean((sales_hat - sales.test)^2)
```
(c) CV for tree pruning

```{r}
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
```
optimal size is 3 - let's see what that means for the test MSE

```{r}
prune.carseats <- prune.tree(tree.carseats, best = 3)
tree.pred <- predict(prune.carseats, Carseats[-train, ])
mean((tree.pred - sales.test)^2)
```

improvement! the one above was 2.736873

(d) now using bagging

(this is the same as below with $m=p$)

```{r}
library(randomForest)
proc <- function(mtry) {
set.seed(1)
bag.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = mtry, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats[-train, ])
mean((yhat.bag - sales.test)^2)
}
proc(dim(Carseats)[2]-1)
```


(e) now using random forest

```{r}
proc(floor(sqrt(dim(Carseats)[2])))
```

it's getting lower!

but let's see for various values of `mtry`:
```{r}
for (i in seq(2, dim(Carseats)[2]-1)) {
  print(c(i,proc(i)))
}
```
lowest MSE is when `mtry=4`

(f) BART

```{r}
library(BART)
x <- Carseats[, 2:12]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```

lowest of them all!

### 8.4.9

(a) train set with 800 observations
```{r}
library(ISLR2)
set.seed(2)
train <- sample(1:nrow(OJ), 800)
```

(b) fit a tree with `Purchase` as the predictor

first using `rpart` 'cause it's cool:
```{r}
tree.OJ <- rpart(Purchase ~ ., data=OJ, subset=train)
summary(tree.OJ)
```
then using `tree` 'cause that's what we need:
```{r}
tree.OJ2 <- tree(Purchase ~ ., OJ, subset = train)
summary(tree.OJ2)
```

training error rate: 15.8%
number of terminal nodes: 9

`rpart` already does CV + best height!
```{r}
printcp(tree.OJ)
```

(c) tree obj, pick a node
```{r}
tree.OJ2
```
looking at node 9 for instance:
```
LoyalCH > 0.035047 116  106.60 MM ( 0.17241 0.82759 ) *
```
details are:
  - it's a terminal node
  - it contains 116 observations
  - deviance is 106.6
  - overall prediction is MM (Minute Maid)
  - 17% take Citrus Hill, 83% take Minute Maid
  

(d) plot! this is just nicer with `rpart.plot`
```{r}
library(rpart.plot)
rpart.plot(tree.OJ, main = "Rpart FTW", type=3)
```

(e) confusion matrix
```{r}
oj.test <- OJ[-train,]
tree.pred <- predict(tree.OJ2, oj.test, type = "class")
table(tree.pred, oj.test$Purchase)
```
```{r}
(148+70)/dim(oj.test)[1]
```
So 80% accuracy on the test data

(f) using `cv.tree` for optimal size
```{r}
tree.OJ2.cv <- cv.tree(tree.OJ2, FUN = prune.misclass)
tree.OJ2.cv
```
tree of size 4 has the lowest cross-validation error.

kind of lines up with `rpart`?
```{r}
printcp(tree.OJ)
```

(g) tree size on the x-axis and cross-validated classification error rate on the y-axis
```{r}
library(ggplot2)
df <- data.frame(size=tree.OJ2.cv$size, dev=tree.OJ2.cv$dev)
ggplot(df, aes(x=size,y=dev, color=dev)) + geom_point() + scale_x_continuous(breaks = seq(0, 10, by = 1))
```

(h) lowest cross-validated classification error rate
that's for a tree of size 4

(i) prune tree using the above
```{r}
prune.OJ2 <- prune.misclass(tree.OJ2, best = 4)
```
(j) compare pruned vs unpruned - training error rate
```{r}
tree.pred <- predict(prune.OJ2, OJ[train,], type = "class")
table(tree.pred, OJ[train,]$Purchase)
```
training error rate on the pruned tree is:
```{r}
1-(455+210)/dim(OJ[train,])[1]
```

(k) compare pruned vs unpruned - test error rate
```{r}
tree.pred <- predict(prune.OJ2, oj.test, type = "class")
table(tree.pred, oj.test$Purchase)
```
```{r}
1-(149+65)/dim(oj.test)[1]
```

for a test error rate (pruned) of 21%

### 8.4.10

(a) clean-up and transform
```{r}
library(ISLR2)
attach(Hitters)
H2 <- Hitters[!is.na(Hitters$Salary),]
dim(Hitters)
dim(H2)
H2$Salary = log(H2$Salary)
```
(b) training set
```{r}
train <- seq(1,200)
train.set <- H2[train,]
test.set <- H2[-train,]
```
(c) boosting
```{r}
library(gbm)
set.seed(1)
lambdas <- seq(0, 0.1, 0.005)
training_mses <- c()
for (shrink in lambdas) {
  boost.h2 <- gbm(Salary ~ ., data=train.set, n.trees=1000, shrinkage=shrink)
  yhat.boost <- predict(boost.h2, data=train.set, n.trees=1000)
  tmse <- mean (( yhat.boost - train.set$Salary)^2)
  training_mses <- c(training_mses, tmse)
}
```
now for the plot:
```{r}
library(ggplot2)
df_train <- data.frame(lambdas=lambdas, training_mses=training_mses)
ggplot(df_train, aes(x=lambdas,y=training_mses, color=training_mses)) + geom_point()
```

(d) now the same, for for test MSE
```{r}
testing_mses <- c()
for (shrink in lambdas) {
  boost.h2 <- gbm(Salary ~ ., data=train.set, n.trees=1000, shrinkage=shrink)
  yhat.boost <- predict(boost.h2, newdata=test.set, n.trees=1000)
  tmse <- mean (( yhat.boost - test.set$Salary)^2)
  testing_mses <- c(testing_mses, tmse)
}
```

```{r}
df_test <- data.frame(lambdas=lambdas, testing_mses=testing_mses)
ggplot(df_test, aes(x=lambdas,y=testing_mses, color=testing_mses)) + geom_point()
```
(e) vs 2 regression approaches in chap 3 and 6 - what are those?

(f) ?

(g) bagging

```{r}
library(randomForest)
set.seed(1)
bag.hitters <- randomForest(Salary ~ ., data=train.set)
yhat.bag <- predict(bag.hitters , newdata = test.set)
mean(( yhat.bag - test.set$Salary)^2)
```

for a test MSE of 22%

?