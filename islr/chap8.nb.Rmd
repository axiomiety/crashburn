---
title: "ISLR Chapter 8"
output: html_notebook
---

# 8.3.1

setup
```{r}
library(tree)
library(ISLR2)
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
Carseats <- data.frame(Carseats, High)
```

```{r}
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
```

note the _residual mean deviance_. 373 is 400-27 (number of terminal nodes)

plots!
```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

obj itself shows the splits details: split, n, deviance, yval, (yprob)
```{r}
tree.carseats
```

splitting the data into a training and testing set:
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

77% accuracy:
```{r}
(104+50)/(104+33+13+50)
```

now with pruning, let's  - we can tweak the selection criteria via `FUN`:
```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
```


plots:
```{r}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

pruning in action:
```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

```{r}
(97+58)/(97+25+20+58)
```

# 8.3.2

regression trees
```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```
splits
```{r}
tree.boston
```

plots!
```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```
same but with a bigger tree?
```{r}
bigtree.boston <- tree(medv ~ ., Boston, subset = train, control = tree.control(nobs = length(train), mindev = 0))
plot(bigtree.boston)
text(bigtree.boston, pretty = 0)
```
ok i need to fix that plot...

using CV for the ideal number of terminal nodes
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

pruning
```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```
unpruned trees to make predictions on the test set:
```{r}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)

```
# 8.3.3

using all 12 predictors at each split of the tree
```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, importance = TRUE)
bag.boston
```
perf is much better than the pruned tree:
```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```
size of the forest matters:
```{r}
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

this gives an indication of how important certain variables are:
```{r}
importance(rf.boston)
varImpPlot(rf.boston)
```
# 8.4.7

test classification error / number of trees for mtry = 6, ntree = 25 and ntree = 500.

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)

get_test_error <- function(mtry, ntree) {
  bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry, ntree = ntree)
  yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
  mean((yhat.bag - boston.test)^2)
}
# build a df of variables
df <- data.frame(expand.grid(seq(2,10),seq(50,500,50)))
names(df) <- c('mtry','ntree')
df['tmse'] <- mapply(get_test_error, df$mtry, df$ntree)
```

now for a plot
```{r}
library(ggplot2)
ggplot(df, aes(x=ntree,y=tmse,color=mtry)) + geom_point()
```
# 8.4.8

predicting `Sales` from `Carseats`

(a) split the data
```{r}
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
```

(b) regression tree - inc MSE
```{r}
# seems we should be doing that with rpart!
tree.carseats <- tree(Sales ~ ., Carseats, subset = train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

same thing with `rpart`:
```{r}
library(rpart)
library(rpart.plot)
tree.carseats2 <- rpart(Sales ~ ., data=Carseats, subset=train)
rpart.plot(tree.carseats2, main = "Rpart FTW", type=3)
```

test MSE
```{r}
sales_hat <- predict(tree.carseats, newdata = Carseats[-train, ])
sales.test <- Carseats[-train, "Sales"]
mean((sales_hat - sales.test)^2)
```
(c) CV for tree pruning

?