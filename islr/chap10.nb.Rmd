---
title: "ISLR Chapter 10"
output:
  html_document:
    df_print: paged
---

# 10.9.1

```{r}
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n/3)
testid <- sample(1:n, ntest)

lfit <- lm(Salary ~., data=Gitters[-testid,])
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid,],mean(abs(lpred - Salary)))
```

lasso:
```{r}
# -1 to omit the intercept, scale for columns with zero mean and unit variance
x <- scale(model.matrix(Salary ~ . -1, data=Gitters))
y <- Gitters$Salary
library(glmnet)
cvfit <- cv.glmnet(x[-testid,], y[-testid], type.measure = "mae")
cpred <- predict(cvfit, x[testid,], s="lambda.min")
mean(abs(y[testid] - cpred))
```

and now NN!

(the trick to getting this working was to `install_tensorflow()` first, then `install_keras()` - make sure you have done `brew install cmake` first)
```{r}
library(keras)
library(tensorflow)
modnn <- keras_model_sequential() %>%
  layer_dense(units = 50, activation="relu", input_shape=ncol(x)) %>%
  layer_dropout(rate=0.4) %>%
  layer_dense(units=1)
```

a single pipe explanation
```{r}
x <- model.matrix(Salary ~ . -1, data=Gitters) %>% scale()
```

controlling the fitting algo:
```{r}
modnn %>% compile(loss = "mse",
  optimizer = optimizer_rmsprop(),
  metrics = list("mean_absolute_error")
)
```

fitting the model (running multiple times improves the fit)
```{r}
history <- modnn %>% fit(
  x[-testid,], y[-testid], ecpohs=1500, batch_size = 32,
  validation_data = list(x[testid,],y[testid])
)
plot(history)
```

pred:
```{r}
npred <- predict(modnn, x[testid,])
mean(abs(y[testid]-npred))
```