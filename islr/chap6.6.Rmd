---
title: "ISLR 6.6"
output: html_notebook
---

### 6.6.6

(a)

```{r}
beta1 <- seq(-2,2,0.1)
y1 <- 1
lambda <- 2
ys <- (y1-beta1)^2 + lambda*beta1^2
plot(beta1, ys)
minimisedBeta <- y1/(1+lambda)
points(minimisedBeta, y1, col="red")
```

(b)

```{r}
beta1 <- seq(-3,3,0.1)
y1 <- 1.5
lambda <- 2
ys <- (y1-beta1)^2 + lambda*abs(beta1)
plot(beta1, ys)
minimisedBeta1 <- y1-lambda/2
minimisedBeta2 <- y1+lambda/2
points(c(minimisedBeta1, minimisedBeta2,0), c(y1,y1,y1), col="red")
```

### 6.6.8

(a)
```{r}
set.seed(1)
n <- 100
x <- rnorm(n)
noise <- rnorm(n)
```

(b)

```{r}
beta0 <- 0.5
beta1 <- 1
beta2 <- 2
beta3 <- 3
y <- beta0 + predictor*beta1 + predictor^2 *beta2 + predictor^3 *beta3 + noise
df <- data.frame(y,x)
```

(c)

```{r}
library(leaps)
regfit.full <- regsubsets(y ~ poly(x, 10, raw=TRUE), data=df, nvmax=10 )
reg.summary <- summary(regfit.full)
reg.summary$rsq
```

```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables",ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",ylab = "Adjusted RSq", type = "l")
num_r2 <- which.max(reg.summary$adjr2)
points(num_r2, reg.summary$adjr2[num_r2], col = "red", cex = 2,pch = 20)

plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
num_cp <- which.min(reg.summary$cp)
points(num_cp, reg.summary$cp[num_cp], col = "red", cex = 2,pch = 20)

num_bic <- which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
points(num_bic, reg.summary$bic[num_bic], col = "red", cex = 2, pch = 20)
```
idk what these plots mean...

```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```

(d)

fwd selection
```{r}
doitall <- function(method) {
  regfit <- regsubsets(y ~ poly(x, 10), data=df, nvmax=10, method=method)
  reg.summary <- summary(regfit)
  print(reg.summary$rsq)
  par(mfrow = c(2, 2))
  plot(reg.summary$rss, xlab = "Number of Variables",ylab = "RSS", type = "l")
  plot(reg.summary$adjr2, xlab = "Number of Variables",ylab = "Adjusted RSq", type = "l")
  num_r2 <- which.max(reg.summary$adjr2)
  points(num_r2, reg.summary$adjr2[num_r2], col = "red", cex = 2,pch = 20)
  
  plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
  num_cp <- which.min(reg.summary$cp)
  points(num_cp, reg.summary$cp[num_cp], col = "red", cex = 2,pch = 20)
  
  num_bic <- which.min(reg.summary$bic)
  plot(reg.summary$bic, xlab = "Number of Variables",
  ylab = "BIC", type = "l")
  points(num_bic, reg.summary$bic[num_bic], col = "red", cex = 2, pch = 20)
}
```

forward!
```{r}
doitall("forward")
```

backwards!


```{r}
doitall("backward")
```

(e)

lasso!
```{r}
library(glmnet)
set.seed(1)
x2 <- model.matrix(y ~ poly(x, 10, raw=TRUE), df)[, -1]
train <- sample(1:nrow(x2), nrow(x2) / 2)
test <- (-train)
y.test <- y[test]
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x2[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
cv.out <- cv.glmnet(x2[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
print(bestlam)
lasso.pred <- predict(lasso.mod, s = bestlam,newx = x2[test, ])
mean((lasso.pred - y.test)^2)
```

```{r}
out <- glmnet(x2, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:11, ]
lasso.coef
```

(f)

```{r}
set.seed(1)
beta0 <- 0.5
beta7 <- 2
newy <- beta0 + beta7*x^7 + noise
dff <- data.frame(newy, x)
```

best subset
```{r}
regfit.full <- regsubsets(newy ~ poly(x, 10, raw=TRUE), data=dff, nvmax=10 )
reg.summary <- summary(regfit.full)
reg.summary$rsq
```

lasso
```{r}
library(glmnet)
set.seed(1)
x2 <- model.matrix(newy ~ poly(x, 10, raw=TRUE), dff)[, -1]
train <- sample(1:nrow(x2), nrow(x2) / 2)
test <- (-train)
y.test <- newy[test]
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x2[train,], newy[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

that plot is interesting - the norm tapers out at the actual value of the coefficient o_O

### 6.6.9

(a) Split the data set into a training set and a test set.
```{r}
library(ISLR2)
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(College), replace = TRUE)
test <- (!train)
```

(b) Fit a linear model using least squares on the training set, and
report the test error obtained.

```{r}
lm.fit = lm(Apps ~ ., data=College, subset=train)
summary(lm.fit)
y_pred = predict(lm.fit, newdata = College[test,])
y_act = College$Apps[test]
mean((y_pred-y_act)^2)
```
(c) Fit a ridge regression model on the training set, with λ chosen
by cross-validation. Report the test error obtained.

```{r}
# we need a matrix for our model
x <- model.matrix(Apps ~ ., College)[, -1]
y <- College$Apps
# create the linear space for lambda
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod, newx=x[test,], s=bestlam)
y.test <- College$Apps[test]
mean((ridge.pred-y.test)^2)
```

MSE is better than Least Square.

(d) Fit a lasso model on the training set, with λ chosen by cross- validation. Report the test error obtained, along with the num-
ber of non-zero coefficient estimates.
```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)
```

And... worse than the above Least Squares. It did drop some coefs though:
```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)
lasso.coef
```

(e) Fit a PCR model on the training set, with M chosen by cross-
validation. Report the test error obtained, along with the value
of M selected by cross-validation.
```{r}
library(pls)
pcr.fit <- pcr(Apps ~ ., data = College, subset = train, scale = TRUE , validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
```

Now use with the lowest $M$:
```{r}
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 16)
mean((pcr.pred - y.test)^2)
```

Which looks huge!

(f) Fit a PLS model on the training set, with M chosen by cross-
validation. Report the test error obtained, along with the value
of M selected by cross-validation.
```{r}
pls.fit <- plsr(Apps ~ ., data = College, subset = train, scale= TRUE , validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
```

So $M=6$:
```{r}
pls.pred <- predict(pls.fit, x[test, ], ncomp = 6)
mean((pls.pred - y.test)^2)
```
Which is still much bigger!

(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

Sounds like in our example, ridge takes the cake.

### 6.6.10

(a)

```{r}
set.seed(1)
n <- 1000
p <- 20
x <- matrix(rnorm(n*p),nrow = n, ncol=p)
b <- runif(p)
b[1] <- 0
b[4] <- 0
b[11] <- 0
b[18] <- 0
y <- x*b+rnorm(n)
```

(b) 100 training, 900 test
```{r}
set.seed(1)
train <- sample(1:n, 100)
test <- seq(1,n)[-train]
```

(c) Perform best subset selection on the training set, and plot the
training set MSE associated with the best model of each size.

```{r}
library(leaps)
df.train <- data.frame(y=y[train], x=x[train,])
regfit.full <- regsubsets(y ~ ., data=df.train, nvmax=p)
summary(regfit.full)
```

let's calculate the MSE on the train set for the best set for each model:
```{r}
train.mat <- model.matrix(y ~ ., data = df.train)
val.errors <- rep(NA, p)
for (i in 1:p) {
  coefi <- coef(regfit.full, id = i)
  pred <- train.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((y[train] - pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.full, 7)
```

plot:
```{r}
plot(val.errors, xlab="num preds")
```

(d) Plot the test set MSE associated with the best model of each

let's calculate the MSE on the test set:
```{r}
df.test <- data.frame(y=y[test], x=x[test,])
test.mat <- model.matrix(y ~ ., data = df.test)
val.errors <- rep(NA, p)
for (i in 1:p) {
  coefi <- coef(regfit.full, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((y[test] - pred)^2)
}
val.errors
which.min(val.errors)
```

and do the plot:
```{r}
plot(val.errors, xlab="num preds")
```

So the best is just using 1 predictor o_O - which is essentially `b[1]` which was set to 0...

(e) For which model size does the test set MSE take on its minimum
value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.

Er... have to play around with the sample...

```{r}
# mess around with the sampling/seeds
```

(f) i'll need to regenerate things for this...

(g)

```{r}
statistic <- rep(NA, p)
for (i in 1:p) {
  coefi <- coef(regfit.full, id = i)
  statistic[i] <- sum((b[seq(1:i)]-coefi[-1])^2)
}
plot(statistic, xlab="p")
```

### 6.6.11

(a) using `Boston`, trying out a variety of models.

preliminaries

```{r}
attach(Boston)
n <- nrow(Boston)
set.seed(1)
train <- sample(1:n, 100)
test <- seq(1,n)[-train]
x <- model.matrix(crim ~ ., Boston)[, -1]
y <- Boston$crim
p <- ncol(Boston)-1 # don't count crim!
```

using best subset:

```{r}
library(leaps)
df.train <- data.frame(y=y[train], x=x[train,])
regfit.full <- regsubsets(y ~ ., data=df.train, nvmax=p)
summary(regfit.full)
```

```{r}
train.mat <- model.matrix(y ~ ., data = df.train)
val.errors <- rep(NA, p)
for (i in 1:p) {
  coefi <- coef(regfit.full, id = i)
  pred <- train.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((y[train] - pred)^2)
}
val.errors
min.idx <- which.min(val.errors)
min.idx
val.errors[min.idx]


```
ok so this uses all predictors!

using lasso:

```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.coef <- predict(out, type = "coefficients", s = bestlam)
lasso.coef
lasso.pred.train <- predict(lasso.mod, s = bestlam, newx = x[train, ])
mean((lasso.pred.train - Boston$crim[train])^2)

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)
```
the MSE is pretty good, comparable to best subset! some of the predictors have tiny values, very close to zero. the intercept is huge though...


using ridge:

```{r}
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred.train <- predict(ridge.mod, s = bestlam, newx = x[train,])
mean((ridge.pred.train-Boston$crim[train])^2)

ridge.pred <- predict(ridge.mod, newx=x[test,], s=bestlam)
y.test <- Boston$crim[test]
mean((ridge.pred-y.test)^2)
```
so the MSE is slightly higher than best subset.

using PCR:

```{r}
library(pls)
pcr.fit <- pcr(crim ~ ., data = Boston[train,], scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
```

```{r}
pcr.pred <- predict(pcr.fit, x[train, ], ncomp = 12)
mean((pcr.pred - Boston$crim[train])^2)
```
it's identical to best subset! guess it makes sense since we're using all components

(b) which models perform best on the test (not training) data?

```{r}

# PCR
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 12)
mean((pcr.pred - Boston$crim[test])^2)
```

(c) looking at the coefficients of the chosen model(s), how many features/predictors do they use?

sadly all :(