---
title: "3.7 - Applied Exercises"
output:
  html_document:
    df_print: paged
---

# 8

## a

```{r}
library(ISLR2)
lm.fit <- lm(mpg ~ horsepower, data=Auto)
summary(lm.fit)
```

### Is there a relationship between the predictor and the response?

About 60% of the variance in `mpg` can be attributed to `horsepower` - and the F-statistic is such that we'd reject the null hypothesis (that is, there is no relationship)

### How strong is the relationship between the predictor and the response?

I'm not sure how to qualify "strong" - is that the slope? Because if so it isn't.

### Is the relationship between the predictor and the response positive or negative?

Negative - an increase in `horsepower` lowers `mpg`

### What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?

```{r}
predict(lm.fit, data.frame(horsepower=c(98)), interval = 'confidence')
predict(lm.fit, data.frame(horsepower=c(98)), interval = 'prediction')
```
As expected, the `prediction` interval is larger.

## b

```{r ggplot}
library(ggplot2)
ggplot(Auto, aes(x=horsepower, y=mpg)) + geom_point(size=2, shape=23) + geom_abline(intercept = lm.fit$coefficients[1], slope = lm.fit$coefficients[2], color="red")
```

## c

```{r plot}
plot(lm.fit)
```

Potential issues:
  - residuals aren't uniformly distributed, which suggests a non-linear relationship
  - the residuals vs leverage plot identifies some points that would impact the regression line if they were removed
  
```{r}
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto)
summary(lm.fit2)
```

```{r plot2}
plot(lm.fit2)
```

# 9

## a

```{r}
library(GGally)
ggpairs(subset(Auto, select=-c(name)), aes(colour=as.factor(origin)))
```

## b

```{r}
cor(subset(Auto, select=-c(name)))
```

## c

```{r}
lm.fit <- lm(mpg ~ ., data=subset(Auto, select=-c(name)))
summary(lm.fit)
```

There's a clear relationship between predictors and the response, with R^2 explaining 82% of the variance. 
Some predictors are not significant like `horsepower` or `acceleration`.
`year` is statistically siginifcant given the p-value.

## d

```{r}
plot(lm.fit)
```
The Residuals vs Leverage plot indicates a few observations with very high leverage.

TThe Residuals vs Fitted plot isn't uniform, indicating the true relationship is unlikely to be linear.

## e

```{r}
lm.fit_multi <- lm(mpg ~ .^2, data=subset(Auto, select=-c(name)))
summary(lm.fit_multi)
```

Note the difference in `R` with `*` and `:` (the former is additive + interaction, the other is interaction only).

Only acceleration and origin seem to be statistically significant?

## f

```{r}
lm.fit <- lm(mpg ~ weight + acceleration + displacement, data=subset(Auto, select=-c(name)))
summary(lm.fit)
lm.fit1 <- lm(mpg ~ weight + acceleration + I(log(displacement)), data=subset(Auto, select=-c(name)))
summary(lm.fit1)
```

`log` of displacement gives an increased R^2 - seems `acceleration` is fairly unsignificant!

# 10

## a

```{r}
library(ISLR2)
lm.fit <- lm(Sales ~ Price + Urban + US, data=Carseats)
summary(lm.fit)
```

## b

`Urban` and `US` are both qualitative.

`Price` is negatively correlated with sales, indicating a higher price doesn't lead to higher sales.

`US` is positive, indicating higher sales in US-based stores.

## c

$f(X) = 13.04 -0.54X_1 -0.022X_2 + 1.2X_3$

## d

We can reject the null hypothesis on all predictors except `Urban` as it's not statistically significant.

```{r}
summary(lm(Sales ~ Urban, data=Carseats))
```

## e

```{r}
lm.carseats2 <- lm(Sales ~ Price + US, data=Carseats)
summary(lm.carseats2)
```
Smaller RSE than with `Urban`

## f

Very poorly! We only explain about 24% of the variance.

## g

```{r}
confint(lm.carseats2)
```
## h

```{r}
plot(lm.carseats2)
```
There is some evidence of high leverage - but the Residuals vs Fitted plot is relatively uniform.

# 11

## a

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
lm.fit <- lm(y ~ 0 + x)
summary(lm.fit)
```

## b

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
lm.fit <- lm(x ~ 0 + y)
summary(lm.fit)
```

## c

$R^2$ is identical to the other way around! Residuals are smaller though - RSE is very different! (though I recall reading it's scale-dependent, so whilst it does look significant it isn't)

## d

```{r}
n <- length(x)
top <- sqrt(n-1) * sum(x*y)
bottom <- sqrt(sum(x^2)*sum(y^2)-sum(x*y)^2)
top/bottom
```

## e

## f

# 12

?

# 13

## a

```{r}
set.seed(1)
x <- rnorm(100)
```

## b

```{r}
eps <- rnorm(100, mean=0, sd=0.25)
```

## c

```{r}
y <- -1 + 0.5*x + eps
length(y)
```

$\beta_0$ is -1, $\beta_1$ is 0.5.

## d

```{r}
library(ggplot2)
df <- data.frame(y=y, x=x)
ggplot(df, aes(x=x, y=y)) + geom_point()
```

## e

```{r}
lm.fit <- lm(y~x)
summary(lm.fit)
```
The coeffs are quite close!

## f

```{r}
ggplot(df, aes(x=x, y=y)) + geom_point() +
geom_abline(intercept=coef(lm.fit)[1], slope = coef(lm.fit)[2], colour="red")
```

## g

```{r}
lm.fit2 <- lm(y ~ poly(x, 2))
summary(lm.fit2)
```
Fit is improved somewhat as noted by the modest increase in $R^2$.

## h

```{r}
set.seed(2)
eps2 <- rnorm(100, mean=0, sd=0.05)
y_low <- -1 + 0.5*x + eps2
lm.fit_low <- lm(y_low ~ x)
df_low <- data.frame(y=y_low, x=x)
ggplot(df_low, aes(x=x, y=y)) + geom_point() +
geom_abline(intercept=coef(lm.fit_low)[1], slope = coef(lm.fit_low)[2], colour="red")
```

## i

```{r}
set.seed(2)
eps3 <- rnorm(100, mean=0, sd=0.5)
y_high <- -1 + 0.5*x + eps3
lm.fit_high <- lm(y_high ~ x)
df_high <- data.frame(y=y_high, x=x)
ggplot(df_high, aes(x=x, y=y)) + geom_point() +
geom_abline(intercept=coef(lm.fit_high)[1], slope = coef(lm.fit_high)[2], colour="red")
```

## j

```{r}
confint(lm.fit)
confint(lm.fit_low)
confint(lm.fit_high)
```

# 14

## a

```{r}
set.seed(1)
x1 <- runif (100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

$\beta_0$ = 2, $\beta_1$ = 2, $\beta_2$ = 0.3

## b

```{r}
library(ggplot2)
df <- data.frame(y=x1, x=x2)
ggplot(df, aes(x=x, y=y)) + geom_point()
```

## c

```{r}
lm.fit <- lm(y ~ x1+x2)
summary(lm.fit)
```
The estimates for $\beta_1$ and $\beta_2$ are quite far off, and not statistically signficant.

## d

```{r}
lm.fit_x1 <- lm(y ~ x1)
summary(lm.fit_x1)
```

## e

```{r}
lm.fit_x2 <- lm(y ~ x2)
summary(lm.fit_x2)
```

## f



## g

```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
summary(lm(y~ x1+x2))
summary(lm(y ~ x1))
summary(lm(y ~ x2))
```
It's a high leverage point!

# 15

## a

```{r}
library(ISLR2)
vars <- colnames(Boston)[-1]
lm_results <- lapply(vars, function(col){
    lm_formula <- as.formula(paste0("crim ~ ", col))
    lm(lm_formula, data = Boston)
})
lapply(lm_results, function(x) summary(x))
```

They're mostly significant in all cases except `chas`.

## b

```{r}
lm.fit <- lm(crim ~ ., data=Boston)
summary(lm.fit)
```

Only 3 predictors show up as being significant.

## c

```{r}
c_a <- coef(lm.fit)[-1]
c_b <- unlist(lapply(lm_results, function(x) coef(x)[2]))
df <- data.frame(x=c_b, y=c_a)
ggplot(df, aes(x=c_b,y=c_a)) + geom_point()
df
```

## d

```{r}
# use poly(x, 3)
```