---
title: "ISLR 7"
output:
  html_document:
    df_print: paged
---

### 7.8.1

```{r}
library(ISLR2)
attach(Wage)
```

Coefficients for a 4th degree polynomial:

```{r}
fit <- lm(wage ~ poly(age, 4), data = Wage)
coef(summary(fit))
```
`poly(age, 4)` is a matrix whose columns are linear combinations of `age, age^2, ...`

Found this for the mathematical definition of poly:
> A quick answer is that poly of a vector is x essentially equivalent to the QR decomposition of the matrix whose columns are powers of x 

Some more details [here](https://www.sydney.edu.au/science/chemistry/~mjtj/CHEM3117/Resources/poly_etc.pdf)

```{r}
m <- poly(age, 4)
dim(m)
dim(Wage)
```

```{r}
fit2 <- lm(wage ~ poly(age, 4, raw=TRUE), data = Wage)
coef(summary(fit2))
```

Similarly

```{r}
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)
coef(fit2a)
```
Slightly different way:

```{r}
fit2b <- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage)
coef(fit2b)
```

Prediction:

```{r}
agelims <- range(age)
age.grid <- seq(from = agelims[1], to = agelims[2])
preds <- predict(fit, newdata = list(age = age.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
```

?
```{r}
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey") > title("Degree-4 Polynomial", outer = T)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

?
```{r}
preds2 <- predict(fit2, newdata = list(age = age.grid), se = TRUE)
max(abs(preds$fit - preds2$fit))
```

ANOVA
```{r}
fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
coef(summary(fit.5))
```

Adding `education`:
```{r}
fit.1 <- lm(wage ~ education + age, data = Wage)
fit.2 <- lm(wage ~ education + poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ education + poly(age, 3), data = Wage)
anova(fit.1, fit.2, fit.3)
```

Using `glm`:
```{r}
fit <- glm(I(wage > 250) ~ poly(age, 4), data = Wage, family = binomial)
```

?
```{r}
preds <- predict(fit, newdata = list(age = age.grid), se = T)
summary(preds)
```

?
```{r}
pfit <- exp(preds$fit) / (1 + exp(preds$fit))
se.bands.logit <- cbind(preds$fit + 2 * preds$se.fit,
preds$fit - 2 * preds$se.fit)
se.bands <- exp(se.bands.logit) / (1 + exp(se.bands.logit))
# same as the below?
preds <- predict(fit, newdata = list(age = age.grid), type = "response", se = T)
```

?
```{r}
plot(age, I(wage > 250), xlim = agelims, type = "n", ylim = c(0, .2))
points(jitter(age), I((wage > 250) / 5), cex = .5, pch = "|", col = "darkgrey")
lines(age.grid, pfit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

```{r}
table(cut(age, 4))
fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))
```

?

### 7.8.2

TIL: `bs` stands for `B-splines`. the set of all cubic splines going through those notes is a vector space, and the basis of that vector space are b-splines. so by expressing splines using those basis functions we are uniquely expressing them.

```{r}
library(ISLR2)
attach(Wage)
library(splines)
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se, lty = "dashed")
```
knots at uniformed quantiles:

```{r}
dim(bs(age, knots = c(25, 40, 60)))
dim(bs(age, df = 6)) # 3 internal knots - k = df - degree
attr(bs(age, df = 6), "knots")
```

use `ns` for natural splines (normal cubic splines are cubic outside of the knots on both ends, natural splines are constrained to be linear at the boundaries - meaning the cubic and quadratic coefs are zero):

```{r}
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid), se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred2$fit, col = "red", lwd = 2)
```

using smooth splines (those are the ones with a penality term):
```{r}
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Smoothing Spline")
fit <- smooth.spline(age, wage, df = 16)
fit2 <- smooth.spline(age, wage, cv = TRUE)
lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"),
col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
fit2$df
```

local regression - using a span of 20% and one of 50% (for the observations in the vicinity of $x_0$)
```{r}
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey") > title("Local Regression")
fit <- loess(wage ~ age, span = .2, data = Wage)
fit2 <- loess(wage ~ age, span = .5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)), col = "red", lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = "blue", lwd = 2)
legend("topright", legend = c("Span = 0.2", "Span = 0.5"), col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
```

### 7.8.3

```{r}
library(splines)
library(gam)
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
par(mfrow = c(1, 3))
plot(gam.m3, se = TRUE, col = "blue")
plot.Gam(gam1, se = TRUE, col = "red")
```

comparing the models:
```{r}
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
```
preds:
```{r}
preds <- predict(gam.m2, newdata = Wage)
gam.lo <- gam(
  wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
  data = Wage )
plot(gam.lo, se = TRUE, col = "green")

```
2D-surface!
```{r}
library(akima)
gam.lo.i <- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage)
plot(gam.lo.i)
```
moar
```{r}
gam.lr <- gam(
  I(wage > 250) ~ year + s(age, df = 5) + education,
  family = binomial, data = Wage )
par(mfrow = c(1, 3))
plot(gam.lr, se = T, col = "green")
```

```{r}
table(education , I(wage > 250))
```
all good finally:
```{r}
gam.lr.s <- gam(
I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial , data = Wage ,
subset = (education != "1. < HS Grad")
)
plot(gam.lr.s, se = T, col = "green")
```
### 7.9.3

```{r}
b0 <- 1
b1 <- 1
b2 <- -2
set.seed(123)
xs <- seq(-4,4,length=200)
indic <- ifelse(xs>=1,1,0)
y <- b0+b1*xs+b2*indic*((xs-1)^2)+rnorm(length(xs))
ystraight <- b0+b1*xs

plot(xs,ystraight, col="red",type="l",xlim=c(min(xs),max(xs)),ylim=c(min(y),max(y)))
points(xs,y)
abline(v=-2, col="blue")
abline(v=2, col="blue")
```

### 7.9.4

```{r}
b0 <- 1
b1 <- 1
b2 <- -3
set.seed(123)
xs <- seq(-4,6,length=300)
indic02 <- ifelse(0<=xs & xs<=2,1,0)
indic12 <- ifelse(1<=xs & xs<=2,1,0)
indic34 <- ifelse(3<=xs & xs<=4,1,0)
indic45 <- ifelse(4<=xs & xs<=5,1,0)

y <- b0+b1*(indic02+(xs-1)*indic12)+b2*(indic45+(xs-3)*indic34)+rnorm(length(xs))
#ystraight <- b0+b1*xs

#plot(xs,ystraight, col="red",type="l",xlim=c(min(xs),max(xs)),ylim=c(min(y),max(y)))
plot(xs,y)
points(xs,y)
abline(v=-2, col="blue")
abline(v=2, col="blue")
```

### 7.9.5

3rd vs 4th derivative

(a) $\lambda \to \inf$, smallest training RSS goes to
(b) $\lambda \to \inf$, smallest test RSS goes to
(c) $\lambda=0$, smallest training or test RSS goes to

### 7.9.6

(a) polynomial regression of `wage` vs `age`

```{r}
library(ISLR2)
attach(Wage)
set.seed(45)
numRows <- dim(Wage)[1]
train <- sample(numRows, numRows*0.7)
max_d <- 10

mses <- rep(0, max_d)
fits <- list()
for (d in seq(1,max_d)) {
  lm.fit <- lm(wage ~ poly(age, d), data = Wage, subset=train)
  mses[d] <- mean((wage - predict(lm.fit, Wage))[-train]^2)
  fits[[d]] <- lm.fit
}



fits <- lapply(1:max_d, function(i) lm(wage ~ poly(age, i), data = Wage, subset=train))

bestDegree <- which.min(mses)
bestDegree
do.call("anova", fits)

```

plotting the one with the best degree:

```{r}
fit <- fits[bestDegree][[1]]
agelims <- range(age)
age.grid <- seq(from = agelims[1], to = agelims[2])
preds <- predict(fit, newdata = list(age = age.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-9 Polynomial", outer = T)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

(b) using step functions

```{r}
table(cut(age, 4))
fits <- lapply(2:max_d, function(d) lm(wage ~ cut(age, d), data = Wage, subset=train))
mses <- rep(0, max_d-2)
for (d in seq(1,max_d-1)) {
  mses[d] <- mean((wage - predict(fits[[d]], Wage))[-train]^2)
}

bestDegree <- which.min(mses)
bestDegree
```

same plot as above but with splines
```{r}
fit <- fits[bestDegree][[1]]
agelims <- range(age)
age.grid <- seq(from = agelims[1], to = agelims[2])
preds <- predict(fit, newdata = list(age = age.grid), se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0))
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-9 Polynomial", outer = T)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
```

do the same but for 

### 7.9.7

```{r}
# let's try marit1 as a poly
library(ggplot2)
ggplot(Wage, aes(x=maritl, y=wage)) +  geom_boxplot(fill='green')
ggplot(Wage, aes(x=jobclass, y=wage)) +  geom_boxplot(fill='red')
ggplot(Wage, aes(x=education, y=wage)) +  geom_boxplot(fill='blue')

fit.m2 <- lm(wage ~ poly(maritl,2), data=Wage, subset=train)
fit.m3 <- lm(wage ~ poly(maritl,3), data=Wage, subset=train)
fit.m4 <- lm(wage ~ poly(maritl,4), data=Wage, subset=train)
anova(fit.m2,fit.m3,fit.m4)

# fit.m3 explains most of the variance vs the other 2
summary(fit.m3)
```

do the same for eduaction

```{r}
fit.e2 <- lm(wage ~ poly(education,2), data=Wage, subset=train)
fit.e3 <- lm(wage ~ poly(education,3), data=Wage, subset=train)
fit.e4 <- lm(wage ~ poly(education,4), data=Wage, subset=train)
# can't because we have 5 levels - levels(Wage$education)
#fit.e5 <- lm(wage ~ poly(education,5), data=Wage, subset=train)

anova(fit.e2,fit.e3,fit.e4, fit.e5)
```

### 7.9.8

Using the Auto data-set

```{r}
library(ggcorrplot)
library(ISLR2)
library(GGally)
attach(Auto)
auto.numeric <- subset(Auto, select=-c(name))
corr <- round(cor(auto.numeric), 1)
ggcorrplot(corr, hc.order = TRUE, type = "lower",
   lab = TRUE)
ggpairs(auto.numeric)
```

let's try mpg vs weight:
```{r}
ggplot(Auto, aes(y=mpg, x=weight)) + geom_point()
```

```{r}
set.seed(456)
numRows <- dim(Auto)[1]
train <- sample(numRows, numRows*0.7)
lm.w2 <- lm(mpg ~ poly(weight,2), data=Auto, subset=train)
lm.w3 <- lm(mpg ~ poly(weight,3), data=Auto, subset=train)
lm.w4 <- lm(mpg ~ poly(weight,4), data=Auto, subset=train)
lm.w5 <- lm(mpg ~ poly(weight,5), data=Auto, subset=train)
lm.w6 <- lm(mpg ~ poly(weight,6), data=Auto, subset=train)
anova(lm.w2,lm.w3,lm.w4,lm.w5,lm.w6)
```
meh. maybe a combination of poly models?

### 7.9.9

(a) predicting `nox` using `dis

```{r}
attach(Boston)
set.seed(45)
numRows <- dim(Boston)[1]
train <- sample(numRows, numRows*0.7)
lm.fit <- lm(nox ~ poly(dis, 3), data = Boston, subset=train)
summary(lm.fit)
```

predictions:

```{r}
preds <- predict(lm.fit, newdata=list(dis = Boston$dis[-train]))
plot(Boston$dis[-train]~preds, col="red")
points(Boston$nox[-train],Boston$dis[-train], type="p", pch=23, col="blue")
```

(b) plot polynomials up to 10, print mse for each on *unseen* data

```{r}
max_d <- 11
fits <- lapply(2:max_d, function(d) lm(nox ~ poly(dis, d), data = Boston, subset=train))
mses_train <- rep(0, max_d-1)
mses <- rep(0, max_d-1)
for (d in seq(1,max_d-1)) {
  mses_train[d] <- mean((nox - predict(fits[[d]], Boston))[train]^2)
  mses[d] <- mean((nox - predict(fits[[d]], Boston))[-train]^2)
}
which.min(mses_train)
which.min(mses)
```

now let's plot!

```{r}
library(reshape2)
df <- data.frame(nox=Boston$nox[train])
mses <- rep(0, max_d-1)
for (d in seq(1,max_d-1)) {
  preds <- predict(fits[[d]], Boston)[train]
  df[paste("degree",d,sep="")] <- preds
}
df["actual"] <- Boston$dis[train]
#points(Boston$nox[-train],Boston$dis[-train], type="p", pch=23, col="blue")
#ggplot(data=df, aes(x=actual,y=nox)) + geom_point()
df <- melt(df ,  id.vars = 'nox', variable.name = 'dis')
ggplot(df, aes(nox, value)) +
  geom_point(aes(colour = dis, palette = "jco"))
ggplot(df, aes(nox, value)) +
  geom_line() +
  facet_grid(dis ~ .)
```

(c) using CV for the best poly

already done above with MSEs

(d) bs for splines. how do we choose the knots?

```{r}
library(splines)
dislims <- range(dis)
dis.grid <- seq(from = dislims[1], to = dislims[2])
fit <- lm(nox ~ bs(dis, df=4), data = Boston)
preds <- predict(fit, newdata = list(dis = dis.grid), se = T)
se.bands <- cbind(preds$fit + 2 * preds$se.fit,preds$fit - 2 * preds$se.fit)
plot(dis, nox, xlim = dislims, cex = .5, col = "darkgrey")
lines(dis.grid, preds$fit, lwd = 2, col = "blue")

```

(e) regression spline using a range of degrees of freedom

```{r}
fit.4 <- lm(nox ~ bs(dis, df=4), data = Boston)
fit.5 <- lm(nox ~ bs(dis, df=5), data = Boston)
fit.6 <- lm(nox ~ bs(dis, df=6), data = Boston)
fit.7 <- lm(nox ~ bs(dis, df=7), data = Boston)

preds.4 <- predict(fit.4, newdata = list(dis = dis.grid), se = T)
preds.5 <- predict(fit.5, newdata = list(dis = dis.grid), se = T)
preds.6 <- predict(fit.6, newdata = list(dis = dis.grid), se = T)
preds.7 <- predict(fit.7, newdata = list(dis = dis.grid), se = T)

plot(dis, nox, xlim = dislims, cex = .5, col = "darkgrey")
lines(dis.grid, preds.4$fit, lwd = 2, col = "blue")
lines(dis.grid, preds.5$fit, lwd = 2, col = "red")
lines(dis.grid, preds.6$fit, lwd = 2, col = "green")
lines(dis.grid, preds.7$fit, lwd = 2, col = "yellow")

```
(f) CV on regression

### 7.9.10

Using the `College` data set - response = out-of-state tuition ($Outstate)

```{r}
library(ISLR2)
library(leaps)
attach(College)
set.seed(456)
numRows <- dim(College)[1]
train <- sample(numRows, numRows*0.5)

# forward step-wise selection - c.f. chap6
# we have 17 possible predictors
regfit.fwd <- regsubsets(Outstate ~ ., data = College, subset=train, nvmax = 17, method = "forward")

# now validate against test set
test.mat <- model.matrix(Outstate~., data = College[test,])
 al.errors <- rep(NA, 17)
for(i in 1:17){
    coefficients <- coef(regfit.fwd, id = i)
    pred <- test_mat[,names(coefficients)]%*%coefficients
    mses[i] <- mean((College$Outstate[test] - pred)^2)
}

which.min(mses)
```

looks like `Private` (tis'a binary var) dominates the other predictors.

(b) fit GAM using the above

```{r}
library(gam)
attach(College)
gam.m4 <- gam(Outstate ~ s(Private,4), data = College, subset=train)
# err... unordered factors cannot be used as smoothing variables
```

