---
title: "ISLR chap5.3"
output: html_notebook
---

### 5.3.1 Validation Approach

```{r}
library(ISLR2)c
set.seed(1)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
summary(lm.fit)
```
Compute the MSE:
```{r}
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
```
Try a `poly` - quadratic and cubic:
```{r}
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
```

We can repeat for a differt `seed` which would lead to different estimates - but it still shows evidence for a quadratic model (vs a linear one), and little benefit for a cubic.

### 5.3.2 LOOCV

`glm` performs linear regression by default (and logistic when `family='binomial'`) 
```{r}
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
```
Built-in for CV - we generate the raw and adjusted validation estimates:
> `delta`: A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.

```{r}
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

Example where those 2 numbers do *not* match:
```{r}
#set.seed(17)
cv.error <- rep(0, 10)
# fit polys up to degree 10
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```

### 5.3.3 k-fold

```{r}
set.seed(17)
cv.error.10 <- rep(0, 10)

for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
```

### 5.3.4 Bootstrap

```{r}
alpha.fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
alpha.fn(Portfolio, 1:100)
```

```{r}
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
```

```{r}
boot(Portfolio, alpha.fn, R = 1000)
```

Worked example:
```{r}
boot.fn <- function(data, index)
  coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
```

```{r}
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
```

```{r}
boot(Auto, boot.fn, 1000)
```

```{r}
boot.fn <- function(data, index) {
  coef(
    lm(mpg ~ horsepower + I(horsepower^2), data=data, subset=index)
  )
}
set.seed(1)
boot(Auto, boot.fn, 1000)
summary(
    lm(mpg ~ horsepower + I(horsepower^2), data=Auto)
  )$coef
```