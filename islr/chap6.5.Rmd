---
title: "ISLR chap6.5."
output: html_notebook
---

# 6.5.1 subset selection

```{r}
library(ISLR2)
#View(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```

disregarding entries with no salary:
```{r}
Hitters2 <- na.omit(Hitters)
dim(Hitters2)
```

subset selection using the `leaps` library:
```{r}
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters2)
summary(regfit.full)
```

let's increase the number of variables (default maxes out at 8 it seems):
```{r}
regfit.full2 <- regsubsets(Salary ~ ., data = Hitters2, nvmax = 19)
reg.summary <- summary(regfit.full2)
reg.summary
```

we can see $R^2$ increase monotonically as more vars are added - which is totally expected:
```{r}
names(reg.summary)
reg.summary$rsq
```

time for some plots!
```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
```

for the other adjustments (it's pretty cool, they're calculated up front by `leaps`):
```{r}
plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2, pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2, pch = 20)
```

all 'em plots!
```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
```
step-wise selection:
```{r}
coef(regfit.full, 6)
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters2, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters2, nvmax = 19, method = "backward")
summary(regfit.bwd)
```


```{r}
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

let's do this using CV:
```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters2),
replace = TRUE)
test <- (!train)
regfit.best <- regsubsets(Salary ~ ., data = Hitters2[train, ], nvmax = 19)
test.mat <- model.matrix(Salary ~ ., data = Hitters2[test, ])
```

find the number of variables in the best model:
```{r}
val.errors <- rep(NA, 19)
for (i in 1:19) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((Hitters2$Salary[test] - pred)^2)
}
which.min(val.errors)
coef(regfit.best, which.min(val.errors))
```

making it a fn:
```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
regfit.best <- regsubsets(Salary ~ ., data = Hitters2, nvmax = 19)
coef(regfit.best, 7)
```

using it for $k$-folds
```{r}
k <- 10
n <- nrow(Hitters2)
set.seed(1)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 19,
dimnames = list(NULL, paste(1:19)))
```

```{r}
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ .,data = Hitters2[folds != j, ],nvmax = 19)
for (i in 1:19) {
pred <- predict(best.fit, Hitters2[folds == j, ], id = i)
cv.errors[j, i] <- mean((Hitters2$Salary[folds == j] - pred)^2)
}
}
```

```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")
```

10 vars gives the best!
```{r}
reg.best <- regsubsets(Salary ~ ., data = Hitters2, nvmax = 19)
coef(reg.best, 10)
```

# 6.5.2 Ridge Regression and the Lasso

## Ridge Regression first

Small transform due to how `glmnet` takes its inputs:
```{r}
x <- model.matrix(Salary ~ ., Hitters2)[, -1]
y <- Hitters2$Salary
```

by default vars are standardised (remember Ridge isn't scale equivariant)
```{r}
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```

```{r}
dim(coef(ridge.mod))
doit <- function(idx) {
  print(ridge.mod$lambda[idx])
  print(coef(ridge.mod)[, idx])
  # l2
  sqrt(sum(coef(ridge.mod)[-1, idx]^2))
}

# 0 is the null model?
doit(1)
doit(50)
```

predict for a new value of $\lambda$:
```{r}
predict(ridge.mod, s = 50, type ="coefficients")[1:20, ]
```

we expect those values to be between of indicies 69 and 70 on our grid:
```{r}
ridge.mod$lambda[69]
ridge.mod$lambda[70]
```

let's estimate the test error:
```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
```

now to evaluate the MSE on the test set using $\lambda=4$:
```{r}
# use the training set to build the model
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# use predict
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test)^2)
```

using a model with only the intercept, we predict the mean for each value and compute the MSE as such:
```{r}
mean((mean(y[train]) - y.test)^2)
```

so ours with $\lambda=4$ is better! but let's compare to LS (which essentially is $\lambda=0).

```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ], exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = T, type = "coefficients",
x = x[train, ], y = y[train])[1:20, ]
```

to choose the best $\lambda$, we can use cross-validation.
```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
# note the x-axis is in log scale!
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

corresponding MSE on our *test* sample is:
```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2)
```

refit on the whole data, none of the coefs are zero - no variable selection (expeced, _ridge regression_ doesn't do that):
```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
```

## Now for the Lasso

```{r}
library(glmnet)
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```
why are we plotting coeffs vs L1 norm??

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam,newx = x[test, ])
mean((lasso.pred - y.test)^2)
```

?

```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20, ]
lasso.coef
lasso.coef[lasso.coef != 0]
```

### 6.5.3

```{r}
library(pls)
library(ISLR2)
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters2, scale = TRUE, validation = "CV")
summary(pcr.fit)
```

```{r}
validationplot(pcr.fit, val.type = "MSEP")
```

evaluate perf on the training data:

```{r}
set.seed(1)
pcr.fit <- pcr(Salary ~ ., data = Hitters2, subset = train, scale = TRUE , validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
```
we can find the `ncomp` with the lowest CV using `min(MSEP(pcr.fit)$val)`

calculate MSE:
```{r}
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)
mean((pcr.pred - y.test)^2)
```

fit on the whole data:
```{r}
pcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)
summary(pcr.fit)
```

#### Partial Least Square

```{r}
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters2, subset = train, scale= TRUE , validation = "CV")
summary(pls.fit)
```

plot!
```{r}
validationplot(pls.fit, val.type = "MSEP")
```
Err - best with only 1 component!
```{r}
pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)
mean((pls.pred - y.test)^2)
```

```{r}
pls.fit <- plsr(Salary ~ ., data = Hitters2, scale = TRUE, ncomp = 1)
summary(pls.fit)
```
